In this repository, I explore the effect of model architecture and training protocols on the learned distribution of word embeddings.

I use Rudman et al. (2022)'s IsoScore, as well as previously used metrics, to quantify the degree to which the distribution of embeddings exploit (or fail to exploit) the entirety of the embedding space.
